# ğŸš€ 100å¤©LLMç³»ç»Ÿå­¦ä¹ ä¸å®è·µè®¡åˆ’ | 100-Day LLM Systematic Learning & Practice Plan

## ä¸­æ–‡ç‰ˆ

### ğŸŒŸ é¡¹ç›®æ¦‚è¿°

ä½œä¸ºä¸€åéç§‘ç­çš„å·¥ç§‘å­¦ç”Ÿï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ€æœ¯åŸç†ä¸åº”ç”¨å‰æ™¯å……æ»¡å¥½å¥‡ä¸çƒ­æƒ…ã€‚æœ¬é¡¹ç›®è®°å½•äº†æˆ‘ä¸ºæœŸ100å¤©çš„ç³»ç»Ÿæ€§å­¦ä¹ æ—…ç¨‹â€”â€”ä»é›¶åŸºç¡€åˆ°å…·å¤‡LLMå…¨æ ˆå®è·µèƒ½åŠ›çš„å®Œæ•´è·¯å¾„ã€‚è¿™ä¸ä»…æ˜¯çŸ¥è¯†ç§¯ç´¯çš„è¿‡ç¨‹ï¼Œæ›´æ˜¯å·¥ç¨‹æ€ç»´ã€å­¦ä¹ æ–¹æ³•å’ŒèŒä¸šèƒ½åŠ›çš„ç»¼åˆé”»é€ ã€‚

### ğŸ¯ æ ¸å¿ƒç†å¿µ

**â€œä»¥é¡¹ç›®é©±åŠ¨å­¦ä¹ ï¼Œä»¥æŒ‡æ ‡è¡¡é‡è¿›æ­¥â€**â€”â€”æœ¬è®¡åˆ’çš„æ¯ä¸ªé˜¶æ®µéƒ½è®¾æœ‰æ˜ç¡®çš„å¯äº¤ä»˜æˆæœå’Œé‡åŒ–æŒ‡æ ‡ï¼Œç¡®ä¿å­¦ä¹ è¿‡ç¨‹å¯è§ã€å¯æµ‹ã€å¯å¤ç°ã€‚æˆ‘åšä¿¡ï¼Œåœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œ**å·¥ç¨‹åŒ–èƒ½åŠ›**ä¸**ç†è®ºåŸºç¡€**åŒç­‰é‡è¦ï¼Œå› æ­¤æœ¬è®¡åˆ’ç‰¹åˆ«å¼ºè°ƒç¯å¢ƒé…ç½®ã€ä»£ç è§„èŒƒã€å®éªŒè®°å½•å’Œé¡¹ç›®éƒ¨ç½²ç­‰å®è·µç¯èŠ‚ã€‚

### ğŸ“Š è¯¦ç»†è·¯çº¿å›¾

#### **é˜¶æ®µä¸€ï¼šç¯å¢ƒä¸å·¥ç¨‹ä¹ æƒ¯æ­å»ºï¼ˆç¬¬1-7å¤©ï¼‰**
- **ç›®æ ‡**ï¼šå»ºç«‹å¯å¤ç°çš„å¼€å‘ç¯å¢ƒä¸è§„èŒƒçš„å·¥ç¨‹å·¥ä½œæµ
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. å®ŒæˆLinux/WSL + CUDA(å¦‚éœ€) + Python(Conda/uv)ç¯å¢ƒé…ç½®ï¼Œå¯¼å‡º`environment.yml`/`requirements.txt`
  2. æŒæ¡Gitæ ¸å¿ƒå·¥ä½œæµï¼ˆclone/branch/commit/PR/rebaseï¼‰ï¼Œåœ¨GitHubåˆ›å»ºå…¬å¼€ä»“åº“å¹¶å®Œæˆâ‰¥10æ¬¡è§„èŒƒæäº¤
  3. æ­å»ºä¸ªäººå­¦ä¹ çœ‹æ¿ï¼ˆNotion/Obsidian/Trelloï¼‰ï¼Œåˆ¶å®š100å¤©é‡Œç¨‹ç¢‘ä¸æ¯å‘¨å¤ç›˜æ¨¡æ¿

#### **é˜¶æ®µäºŒï¼šPythonå·¥ç¨‹åŒ–ä¸æ•°æ®å¤„ç†ï¼ˆç¬¬8-18å¤©ï¼‰**
- **ç›®æ ‡**ï¼šä»â€œè„šæœ¬ç¼–å†™â€å‡çº§ä¸ºâ€œå¯ç»´æŠ¤æ¨¡å—å¼€å‘â€
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. å®ŒæˆPythonæ ¸å¿ƒç‰¹æ€§ï¼ˆæ•°æ®ç»“æ„ã€OOPã€å¼‚å¸¸å¤„ç†ã€ç±»å‹æ ‡æ³¨ã€è¿­ä»£å™¨/ç”Ÿæˆå™¨ï¼‰ç³»ç»Ÿç»ƒä¹ â‰¥50é¢˜
  2. æŒæ¡NumPy/Pandas/Matplotlibï¼Œäº§å‡ºåŒ…å«â‰¥3å¼ å¯è§†åŒ–å›¾è¡¨+â‰¥3ä¸ªç»Ÿè®¡æŒ‡æ ‡çš„å°å‹æ•°æ®åˆ†ææŠ¥å‘Š
  3. ä¸ºä»“åº“é›†æˆpytestå•å…ƒæµ‹è¯•â‰¥10ä¸ªï¼Œé…ç½®ruff/blackæ ¼å¼åŒ–ä¸pre-commité’©å­ï¼Œå®ç°GitHub Actions CIæµæ°´çº¿

#### **é˜¶æ®µä¸‰ï¼šè®¡ç®—æœºåŸºç¡€ç´ å…»è¡¥é½ï¼ˆç¬¬19-30å¤©ï¼‰**
- **ç›®æ ‡**ï¼šæ„å»ºLLMå·¥ç¨‹å¿…éœ€çš„åº•å±‚çŸ¥è¯†ä½“ç³»
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. æŒæ¡Linuxå¸¸ç”¨å‘½ä»¤ã€shellè„šæœ¬ç¼–å†™ã€è¿›ç¨‹/å†…å­˜/æ–‡ä»¶æƒé™ç®¡ç†ï¼Œå®Œæˆå®æ“æ¸…å•â‰¥30é¡¹
  2. å®ç°å¹¶è§£æç»å…¸æ•°æ®ç»“æ„ä¸ç®—æ³•ï¼ˆæ ˆ/é˜Ÿåˆ—/å“ˆå¸Œ/å †/å¹¶æŸ¥é›†/æ’åº/æœç´¢ï¼‰ï¼ŒLeetCodeé£æ ¼é¢˜ç›®ç´¯è®¡â‰¥40é¢˜
  3. æ’°å†™â€œLLMæœåŠ¡ç½‘ç»œé“¾è·¯â€æŠ€æœ¯ç¬”è®°ï¼Œæ¶µç›–HTTP/HTTPSã€RESTã€WebSocketç­‰ï¼Œä½¿ç”¨curlå¤ç°â‰¥5ä¸ªAPIè°ƒç”¨ç¤ºä¾‹

#### **é˜¶æ®µå››ï¼šæœºå™¨å­¦ä¹ æ ¸å¿ƒä¸å®éªŒæ–¹æ³•ï¼ˆç¬¬31-42å¤©ï¼‰**
- **ç›®æ ‡**ï¼šå»ºç«‹å¯å¤ç°çš„MLå®éªŒæµç¨‹ä¸è¯„ä¼°ç›´è§‰
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè®­ç»ƒâ‰¥3ä¸ªå¯¹æ¯”æ¨¡å‹ï¼ˆLRã€XGBoostã€MLPï¼‰ï¼Œå½¢æˆå®Œæ•´è¯„ä¼°å¯¹æ¯”è¡¨
  2. äº§å‡ºæ ‡å‡†åŒ–å®éªŒæŠ¥å‘Šï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹é€‰æ‹©ã€è¯„ä¼°æŒ‡æ ‡ä¸ç»“è®ºåˆ†æ
  3. æ·±å…¥ç†è§£è¿‡æ‹Ÿåˆã€æ•°æ®æ³„æ¼ã€ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå„æå‡ºâ‰¥2ç§è§£å†³æ–¹æ¡ˆ

#### **é˜¶æ®µäº”ï¼šæ·±åº¦å­¦ä¹ ä¸PyTorchè®­ç»ƒèŒƒå¼ï¼ˆç¬¬43-54å¤©ï¼‰**
- **ç›®æ ‡**ï¼šæŒæ¡æ¨¡å‹è®­ç»ƒå…¨æµç¨‹ä¸é—®é¢˜è¯Šæ–­èƒ½åŠ›
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. ä½¿ç”¨PyTorchä»é›¶å®ç°åˆ†ç±»æ¨¡å‹ï¼Œé›†æˆDataset/DataLoaderã€è®­ç»ƒå¾ªç¯ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰å®Œæ•´ç»„ä»¶
  2. æ·±å…¥ç†è§£åå‘ä¼ æ’­ã€ä¼˜åŒ–å™¨ã€æ¢¯åº¦è£å‰ªç­‰æ ¸å¿ƒæœºåˆ¶
  3. å®Œæˆâ‰¥2æ¬¡è®­ç»ƒæ’éšœå®è·µï¼Œå®ç°æŒ‡æ ‡æå‡æˆ–è®­ç»ƒæ•ˆç‡ä¼˜åŒ–â‰¥20%

#### **é˜¶æ®µå…­ï¼šLLMåŸºç¡€ä¸Transformeræ·±åº¦è§£æï¼ˆç¬¬55-63å¤©ï¼‰**
- **ç›®æ ‡**ï¼šä»â€œAPIè°ƒç”¨è€…â€è½¬å˜ä¸ºâ€œæœºåˆ¶ç†è§£è€…â€
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. æ’°å†™â‰¥3000å­—Transformeræ ¸å¿ƒæŠ€æœ¯ç¬”è®°ï¼Œæ¶µç›–Self-Attentionã€ä½ç½®ç¼–ç ã€LayerNormç­‰æ ¸å¿ƒæ¨¡å—
  2. å¯¹æ¯”å®ç°â‰¥3ç§è§£ç ç­–ç•¥ï¼ˆgreedy/beam/top-p/top-k/temperatureï¼‰
  3. å®Œæˆå°è§„æ¨¡æ¨¡å‹æ¨ç†å®éªŒï¼Œé‡åŒ–è®°å½•ååã€å»¶è¿Ÿã€æ˜¾å­˜å ç”¨æ€§èƒ½æŒ‡æ ‡

#### **é˜¶æ®µä¸ƒï¼šLLMåº”ç”¨å¼€å‘ï¼šRAGä¸å·¥å…·è°ƒç”¨ï¼ˆç¬¬64-74å¤©ï¼‰**
- **ç›®æ ‡**ï¼šæ„å»ºâ€œé—®é¢˜â†’æ–¹æ¡ˆâ†’å®ç°â†’è¯„ä¼°â€çš„å®Œæ•´åº”ç”¨é—­ç¯
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. æ­å»ºæ”¯æŒâ‰¥200ä»½æ–‡æ¡£çš„RAGåŸå‹ç³»ç»Ÿï¼Œé›†æˆæ–‡æ¡£å¤„ç†ã€å‘é‡æ£€ç´¢ã€ç”Ÿæˆå›ç­”å…¨æµç¨‹
  2. å»ºç«‹ç¦»çº¿è¯„ä¼°ä½“ç³»ï¼ŒåŸºäºè‡ªå»ºâ‰¥50æ¡QAå¯¹è®¡ç®—å¬å›ç‡/å‡†ç¡®ç‡
  3. å¼€å‘å¯äº¤äº’Demoï¼Œç³»ç»Ÿæ”¶é›†â‰¥30æ¡å¤±è´¥æ¡ˆä¾‹ç”¨äºè¿­ä»£ä¼˜åŒ–

#### **é˜¶æ®µå…«ï¼šå¾®è°ƒèƒ½åŠ›ï¼šSFT/LoRA/QLoRAä¸æ•°æ®å·¥ç¨‹ï¼ˆç¬¬75-86å¤©ï¼‰**
- **ç›®æ ‡**ï¼šåœ¨çœŸå®èµ„æºçº¦æŸä¸‹å®Œæˆå¯äº¤ä»˜çš„å¾®è°ƒä»»åŠ¡
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. ä½¿ç”¨LoRA/QLoRAæŠ€æœ¯å®ŒæˆæŒ‡ä»¤å¾®è°ƒå®è·µï¼Œæ”¯æŒå•å¡/å¤šå¡ç¯å¢ƒ
  2. æ„å»ºå¹¶æ¸…æ´—â‰¥2000æ¡é«˜è´¨é‡æŒ‡ä»¤æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„æ•°æ®æ ¡éªŒæµç¨‹
  3. åœ¨è‡ªå»ºè¯„æµ‹é›†ä¸Šå®ç°å…³é”®æŒ‡æ ‡æå‡â‰¥10%ï¼Œä¿å­˜å¯å¤ç°å®éªŒé…ç½®

#### **é˜¶æ®µä¹ï¼šéƒ¨ç½²ä¸å·¥ç¨‹åŒ–ï¼šæ¨¡å‹æœåŠ¡åŒ–ï¼ˆç¬¬87-95å¤©ï¼‰**
- **ç›®æ ‡**ï¼šæŒæ¡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¸æ€§èƒ½ä¼˜åŒ–æŠ€èƒ½
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. åŸºäºvLLM/TGI/llama.cppéƒ¨ç½²æ¨ç†æœåŠ¡ï¼Œæä¾›REST APIä¸æµå¼è¾“å‡º
  2. å®æ–½æ€§èƒ½å‹æµ‹ä¸ä¼˜åŒ–ï¼Œå®ç°P95å»¶è¿Ÿæˆ–ååé‡ä¼˜åŒ–â‰¥20%
  3. é›†æˆå¯è§‚æµ‹æ€§å·¥å…·ï¼Œèƒ½å¤Ÿè¯Šæ–­è¶…æ—¶ã€OOMç­‰å¸¸è§çº¿ä¸Šé—®é¢˜

#### **é˜¶æ®µåï¼šä½œå“é›†ä¸æ±‚èŒå¯¹é½ï¼ˆç¬¬96-100å¤©ï¼‰**
- **ç›®æ ‡**ï¼šå°†æŠ€æœ¯èƒ½åŠ›è½¬åŒ–ä¸ºæœ‰æ•ˆçš„èŒä¸šä¿¡å·
- **å…³é”®æŒ‡æ ‡**ï¼š
  1. å®Œå–„â‰¥2ä¸ªå±•ç¤ºçº§é¡¹ç›®ï¼ˆRAGåº”ç”¨+å¾®è°ƒé¡¹ç›®ï¼‰ï¼ŒåŒ…å«å®Œæ•´æ–‡æ¡£ä¸æ¼”ç¤º
  2. äº§å‡ºLLMå²—ä½å®šåˆ¶ç®€å†ï¼Œå®Œæˆâ‰¥3æ¬¡æ¨¡æ‹Ÿé¢è¯•å¹¶è¾“å‡ºå¤ç›˜æ¸…å•â‰¥30æ¡
  3. åˆ†æâ‰¥30ä¸ªç›®æ ‡å²—ä½JDï¼Œå®Œæˆé¦–è½®æŠ•é€’â‰¥20ä»½å¹¶å»ºç«‹åé¦ˆè·Ÿè¸ªä½“ç³»

### ğŸ› ï¸ å­¦ä¹ æ–¹æ³•è®º

æœ¬è®¡åˆ’é‡‡ç”¨ **â€œä¸‰ç»´å­¦ä¹ æ³•â€**ï¼š
1. **æ·±åº¦ç†è®º**ï¼šé€šè¿‡æ¨å¯¼ã€ç¬”è®°ã€æ•™å­¦å¼è¾“å‡ºå¼ºåŒ–ç†è§£
2. **åŠ¨æ‰‹å®è·µ**ï¼šæ¯ä¸ªçŸ¥è¯†ç‚¹é…å¥—å¯è¿è¡Œçš„ä»£ç ä¸å¯æµ‹é‡çš„ç»“æœ
3. **å·¥ç¨‹è§„èŒƒ**ï¼šä»ç¬¬ä¸€å¤©èµ·åŸ¹å…»ç‰ˆæœ¬æ§åˆ¶ã€æµ‹è¯•ã€æ–‡æ¡£ã€å¯å¤ç°çš„ä¹ æƒ¯

### ğŸ“ˆ é¢„æœŸæˆæœ

- **æŠ€æœ¯å±‚é¢**ï¼šå»ºç«‹ä»æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒåˆ°æœåŠ¡éƒ¨ç½²çš„LLMå…¨æ ˆèƒ½åŠ›
- **å·¥ç¨‹å±‚é¢**ï¼šå½¢æˆä¸¥è°¨çš„å®éªŒæ–¹æ³•è®ºä¸ä»£ç è§„èŒƒæ„è¯†
- **èŒä¸šå±‚é¢**ï¼šæ‰“é€ æœ‰ç«äº‰åŠ›çš„æŠ€æœ¯ä½œå“é›†ä¸å²—ä½åŒ¹é…ç­–ç•¥

### ğŸ¤ é‚€è¯·å‚ä¸

æ— è®ºä½ æ˜¯ï¼š
- ğŸ¤” å¯¹LLMæ„Ÿå…´è¶£ä½†ä¸çŸ¥å¦‚ä½•å¼€å§‹çš„å­¦ä¹ è€…
- ğŸ§ª å¸Œæœ›å‚è€ƒç»“æ„åŒ–å­¦ä¹ è·¯å¾„çš„åŒè·¯äºº
- ğŸ¢ å…³æ³¨å·¥ç¨‹èƒ½åŠ›åŸ¹å…»çš„æŠ€æœ¯å¯¼å¸ˆ
- ğŸ” å¯»æ‰¾æ½œåŠ›äººæ‰çš„æ‹›è˜è€…

éƒ½æ¬¢è¿å…³æ³¨æœ¬é¡¹ç›®è¿›å±•ã€æå‡ºå»ºè®®æˆ–å‚ä¸è®¨è®ºã€‚æ¯ä¸€æ¬¡Starã€Forkæˆ–Issueéƒ½æ˜¯å¯¹æˆ‘å­¦ä¹ æ—…ç¨‹çš„å®è´µæ”¯æŒï¼

---

# ğŸš€ 100-Day LLM Systematic Learning & Practice Plan

## English Version

### ğŸŒŸ Project Overview

As a non-computer science engineering student, I am deeply fascinated by the technical principles and application prospects of Large Language Models (LLMs). This project documents my 100-day systematic learning journeyâ€”a complete path from foundational knowledge to full-stack LLM practical capabilities. This is not merely an accumulation of knowledge but a comprehensive cultivation of engineering thinking, learning methodologies, and professional competencies.

### ğŸ¯ Core Philosophy

**"Project-driven learning, metrics-measured progress"**â€”Each phase of this plan has clear deliverables and quantifiable metrics, ensuring the learning process is visible, measurable, and reproducible. I firmly believe that in the era of artificial intelligence, **engineering capability** is equally important as **theoretical foundation**. Therefore, this plan emphasizes practical aspects such as environment setup, code standards, experiment logging, and project deployment.

### ğŸ“Š Detailed Roadmap

#### **Phase 1: Environment & Engineering Habits Setup (Days 1-7)**
- **Goal**: Establish reproducible development environments and standardized engineering workflows
- **Key Metrics**:
  1. Configure Linux/WSL + CUDA (if needed) + Python (Conda/uv) environment, export `environment.yml`/`requirements.txt`
  2. Master Git core workflow (clone/branch/commit/PR/rebase), create public GitHub repository with â‰¥10 standardized commits
  3. Build personal learning dashboard (Notion/Obsidian/Trello) with 100-day milestones and weekly review templates

#### **Phase 2: Python Engineering & Data Processing (Days 8-18)**
- **Goal**: Advance from "script writing" to "maintainable module development"
- **Key Metrics**:
  1. Complete â‰¥50 systematic exercises on Python core features (data structures, OOP, exception handling, type hints, iterators/generators)
  2. Master NumPy/Pandas/Matplotlib, produce small-scale data analysis report with â‰¥3 visualizations + â‰¥3 statistical metrics
  3. Integrate â‰¥10 pytest unit tests, configure ruff/black formatting with pre-commit hooks, implement GitHub Actions CI pipeline

#### **Phase 3: Foundational Computer Literacy (Days 19-30)**
- **Goal**: Build essential underlying knowledge system for LLM engineering
- **Key Metrics**:
  1. Master Linux commands, shell scripting, process/memory/file permission management, complete â‰¥30 practical tasks
  2. Implement and analyze classic data structures & algorithms (stack/queue/hash/heap/union-find/sorting/search), solve â‰¥40 LeetCode-style problems
  3. Write "LLM Service Network Pipeline" technical notes covering HTTP/HTTPS, REST, WebSocket, reproduce â‰¥5 API call examples using curl

#### **Phase 4: Machine Learning Core & Experimental Methods (Days 31-42)**
- **Goal**: Establish reproducible ML experimental workflows and evaluation intuition
- **Key Metrics**:
  1. Train â‰¥3 comparative models (LR, XGBoost, MLP) on public dataset, create comprehensive evaluation comparison table
  2. Produce standardized experiment report covering data processing, model selection, evaluation metrics, and conclusions
  3. Deeply understand overfitting, data leakage, class imbalance, propose â‰¥2 solutions for each problem

#### **Phase 5: Deep Learning & PyTorch Training Paradigm (Days 43-54)**
- **Goal**: Master end-to-end model training pipeline and problem diagnosis capabilities
- **Key Metrics**:
  1. Implement classification model from scratch using PyTorch, integrate complete components (Dataset/DataLoader, training loop, learning rate scheduling)
  2. Deeply understand backpropagation, optimizers, gradient clipping, and other core mechanisms
  3. Complete â‰¥2 training troubleshooting practices, achieve â‰¥20% metric improvement or training efficiency optimization

#### **Phase 6: LLM Fundamentals & Transformer Deep Dive (Days 55-63)**
- **Goal**: Transform from "API consumer" to "mechanism understander"
- **Key Metrics**:
  1. Write â‰¥3000-word technical notes on Transformer core components (Self-Attention, positional encoding, LayerNorm, etc.)
  2. Compare and implement â‰¥3 decoding strategies (greedy/beam/top-p/top-k/temperature)
  3. Conduct small-scale model inference experiment, quantitatively record throughput, latency, and memory usage metrics

#### **Phase 7: LLM Application Development: RAG & Tool Calling (Days 64-74)**
- **Goal**: Build complete "problemâ†’solutionâ†’implementationâ†’evaluation" application loop
- **Key Metrics**:
  1. Develop RAG prototype supporting â‰¥200 documents, integrating document processing, vector retrieval, and answer generation
  2. Establish offline evaluation system calculating recall/accuracy based on self-built â‰¥50 QA pairs
  3. Develop interactive demo, systematically collect â‰¥30 failure cases for iterative improvement

#### **Phase 8: Fine-tuning Capability: SFT/LoRA/QLoRA & Data Engineering (Days 75-86)**
- **Goal**: Complete deliverable fine-tuning tasks under real resource constraints
- **Key Metrics**:
  1. Practice instruction fine-tuning using LoRA/QLoRA techniques, supporting single/multi-GPU environments
  2. Build and clean â‰¥2000 high-quality instruction data samples with complete validation pipeline
  3. Achieve â‰¥10% key metric improvement on self-built evaluation set, save reproducible experiment configuration

#### **Phase 9: Deployment & Engineering: Model Servicization (Days 87-95)**
- **Goal**: Master production deployment and performance optimization skills
- **Key Metrics**:
  1. Deploy inference service using vLLM/TGI/llama.cpp, provide REST API and streaming output
  2. Conduct performance testing and optimization, achieve â‰¥20% P95 latency reduction or throughput improvement
  3. Integrate observability tools, capable of diagnosing common production issues (timeout, OOM, etc.)

#### **Phase 10: Portfolio & Career Alignment (Days 96-100)**
- **Goal**: Transform technical capabilities into effective career signals
- **Key Metrics**:
  1. Polish â‰¥2 showcase projects (RAG application + fine-tuning project) with complete documentation and demos
  2. Create LLM-position-targeted resume, complete â‰¥3 mock interviews with â‰¥30å¤ç›˜ points output
  3. Analyze â‰¥30 target job descriptions, submit â‰¥20 initial applications and establish feedback tracking system

### ğŸ› ï¸ Learning Methodology

This plan adopts a **"Three-Dimensional Learning Approach"**:
1. **Deep Theory**: Strengthen understanding through derivations, note-taking, and teach-style output
2. **Hands-on Practice**: Pair each knowledge point with runnable code and measurable results
3. **Engineering Standards**: Cultivate habits of version control, testing, documentation, and reproducibility from day one

### ğŸ“ˆ Expected Outcomes

- **Technical Level**: Establish full-stack LLM capabilities from data preparation and model training to service deployment
- **Engineering Level**: Form rigorous experimental methodology and code standardization awareness
- **Career Level**: Build competitive technical portfolio and job matching strategy

### ğŸ¤ Invitation to Participate

Whether you are:
- ğŸ¤” A learner interested in LLMs but unsure where to start
- ğŸ§ª A fellow traveler seeking reference for structured learning paths
- ğŸ¢ A technical mentor focused on engineering skill development
- ğŸ” A recruiter searching for potential talent

You are welcome to follow this project's progress, provide suggestions, or participate in discussions. Every Star, Fork, or Issue is valuable support for my learning journey!

---

**ğŸ“… Start Date**: [2026.01.01] | **ğŸ·ï¸ Tags**: #LLM #LearningJourney #100DaysOfCode #AIEngineering #CareerDevelopment
